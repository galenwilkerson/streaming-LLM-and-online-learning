{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b1f534",
   "metadata": {},
   "source": [
    "# **Tutorial on Streaming LLMs: Inference, Training, and Simultaneous Learning**\n",
    "\n",
    "In this tutorial, we will explore streaming Large Language Models (LLMs) by discussing the concepts of streaming inference, training (online learning), and simultaneous inference and training. We will cover high-level intuitions, core algorithmic details, and provide simple Python implementations for each concept.\n",
    "\n",
    "### **Part 1a: Streaming Inference**\n",
    "\n",
    "#### **High-Level Intuition**\n",
    "\n",
    "Streaming inference refers to the process of handling data that arrives continuously and producing outputs in real-time. Instead of waiting for the entire input data to be available, the model processes smaller chunks of data as they arrive and generates incremental outputs. This approach is crucial for applications like live transcription, chatbots, or real-time monitoring systems, where immediate responses are necessary.\n",
    "\n",
    "#### **Example Use Case**\n",
    "\n",
    "Imagine a live transcription service where speech is being converted to text in real-time. As the speaker continues to talk, the system continuously processes the incoming audio data, updates the context with each new sentence or phrase, and generates the transcription incrementally. This allows the service to produce text almost instantly, maintaining the flow of conversation or presentation.\n",
    "\n",
    "#### **Core Algorithmic Details**\n",
    "\n",
    "1. **Input Chunking:** The continuous data stream is broken into manageable chunks (e.g., sentences or phrases).\n",
    "2. **State Management:** The model maintains and updates a context or state that accumulates information from previous chunks.\n",
    "3. **Incremental Output Generation:** The model generates outputs after processing each chunk, based on the updated state.\n",
    "\n",
    "#### **Python Implementation**\n",
    "\n",
    "Here’s a simple implementation to demonstrate streaming inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9cd09a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: Hello there,\n",
      "Processed: Hello there, I hope you're doing well.\n",
      "Processed: Hello there, I hope you're doing well. Today, we're going to talk about streaming algorithms.\n",
      "Processed: Hello there, I hope you're doing well. Today, we're going to talk about streaming algorithms. Streaming algorithms process data in chunks,\n",
      "Processed: Hello there, I hope you're doing well. Today, we're going to talk about streaming algorithms. Streaming algorithms process data in chunks, allowing for real-time processing and responses.\n"
     ]
    }
   ],
   "source": [
    "# Simulated text stream input\n",
    "text_stream = [\n",
    "    \"Hello there,\",\n",
    "    \"I hope you're doing well.\",\n",
    "    \"Today, we're going to talk about streaming algorithms.\",\n",
    "    \"Streaming algorithms process data in chunks,\",\n",
    "    \"allowing for real-time processing and responses.\"\n",
    "]\n",
    "\n",
    "# Initialize an empty context (state)\n",
    "context = \"\"\n",
    "\n",
    "def process_chunk(chunk, context):\n",
    "    \"\"\"\n",
    "    Processes a single chunk of text, updates the context, and generates output.\n",
    "    \"\"\"\n",
    "    # Update the context by appending the current chunk\n",
    "    new_context = context + \" \" + chunk\n",
    "\n",
    "    # Generate output based on the updated context\n",
    "    output = f\"Processed: {new_context.strip()}\"\n",
    "    \n",
    "    return new_context, output\n",
    "\n",
    "# Process each chunk in the text stream\n",
    "for chunk in text_stream:\n",
    "    context, output = process_chunk(chunk, context)\n",
    "    print(output)  # Print the incremental output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4034fd",
   "metadata": {},
   "source": [
    "### Part 1b: Implementing a Simple Streaming LLM Using GPT-2\n",
    "\n",
    "#### **Introduction**\n",
    "\n",
    "In this tutorial, we'll explore how to implement a simple streaming Large Language Model (LLM) using the GPT-2 model from Hugging Face's `transformers` library. Streaming LLMs are useful in scenarios where data arrives continuously, and immediate processing and output generation are required. Examples include live transcription, chatbots, or any application that needs to handle real-time data streams.\n",
    "\n",
    "We'll simulate a text stream and process it incrementally, maintaining context between chunks and using the model to predict the next word based on the evolving context.\n",
    "\n",
    "#### **Description**\n",
    "\n",
    "The core idea behind streaming LLMs is to handle data that arrives in chunks and generate outputs in real-time. Unlike batch processing, where the entire dataset is processed at once, streaming processing deals with data incrementally. This approach is particularly useful when working with large or continuous data streams.\n",
    "\n",
    "In this implementation, we'll use the pre-trained GPT-2 model to process a series of text chunks as they arrive:\n",
    "\n",
    "1. **Model and Tokenizer Initialization:**\n",
    "   - We load the GPT-2 model and its associated tokenizer, which are pre-trained on a large corpus of text. The model is set to evaluation mode since we are not training but only making predictions.\n",
    "\n",
    "2. **Simulated Text Stream:**\n",
    "   - We define a series of text chunks that simulate data arriving in a stream. Each chunk represents a portion of the overall input text.\n",
    "\n",
    "3. **Context Management:**\n",
    "   - We maintain a context that accumulates the content of all previous chunks. This context is crucial for generating meaningful predictions as it provides the necessary background for the model.\n",
    "\n",
    "4. **Chunk Processing and Prediction:**\n",
    "   - For each incoming chunk, the context is updated, and the model generates a prediction for the next word based on the current context. The prediction is made using greedy decoding, where the model selects the word with the highest probability.\n",
    "\n",
    "5. **Incremental Output Generation:**\n",
    "   - The model's prediction, along with the updated context, is printed after processing each chunk. This demonstrates how the model's understanding evolves as more data is fed into it.\n",
    "\n",
    "By the end of this tutorial, you'll understand how to use a pre-trained language model like GPT-2 in a streaming context, enabling you to build real-time applications that require immediate processing and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "606bf87d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Hello there,\n",
      "Predicted Next Word: I\n",
      "Context: Hello there, I hope you're doing well.\n",
      "Predicted Next Word: \n",
      "Context: Hello there, I hope you're doing well. Today, we're going to talk about streaming algorithms.\n",
      "Predicted Next Word: \n",
      "Context: Hello there, I hope you're doing well. Today, we're going to talk about streaming algorithms. Streaming algorithms process data in chunks,\n",
      "Predicted Next Word: and\n",
      "Context: Hello there, I hope you're doing well. Today, we're going to talk about streaming algorithms. Streaming algorithms process data in chunks, allowing for real-time processing and responses.\n",
      "Predicted Next Word: \n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Simulated text stream input\n",
    "text_stream = [\n",
    "    \"Hello there,\",\n",
    "    \"I hope you're doing well.\",\n",
    "    \"Today, we're going to talk about streaming algorithms.\",\n",
    "    \"Streaming algorithms process data in chunks,\",\n",
    "    \"allowing for real-time processing and responses.\"\n",
    "]\n",
    "\n",
    "# Initialize an empty context (state)\n",
    "context = \"\"\n",
    "\n",
    "def process_chunk(chunk, context, model, tokenizer, max_length=50):\n",
    "    \"\"\"\n",
    "    Processes a single chunk of text using a language model, updates the context, and generates output.\n",
    "    \"\"\"\n",
    "    # Update the context by appending the current chunk\n",
    "    new_context = context + \" \" + chunk\n",
    "\n",
    "    # Ensure the context doesn't exceed the max_length\n",
    "    tokens = tokenizer.tokenize(new_context)\n",
    "    tokens = tokens[-max_length:]  # Keep only the last max_length tokens\n",
    "    new_context = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "    # Tokenize the new context\n",
    "    inputs = tokenizer(new_context, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Generate predictions (inference)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted next word token (greedy decoding)\n",
    "    if outputs.logits.size(1) > 0:\n",
    "        predicted_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1).item()\n",
    "        predicted_word = tokenizer.decode(predicted_token_id).strip()\n",
    "    else:\n",
    "        predicted_word = \"No prediction possible\"\n",
    "\n",
    "    # Generate output based on the updated context\n",
    "    output = f\"Context: {new_context.strip()}\\nPredicted Next Word: {predicted_word}\"\n",
    "    \n",
    "    return new_context, output\n",
    "\n",
    "# Process each chunk in the text stream\n",
    "for chunk in text_stream:\n",
    "    context, output = process_chunk(chunk, context, model, tokenizer)\n",
    "    print(output)  # Print the incremental output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e0cf0",
   "metadata": {},
   "source": [
    "### Top-K sampling\n",
    "\n",
    "The issue of inconsistent next-word predictions can stem from various factors, including the specifics of the context, how the tokenizer splits the text, and inherent limitations of the model, especially for shorter or less predictable contexts.\n",
    "Advanced Solution: Top-K Sampling\n",
    "\n",
    "One way to potentially improve the quality of predictions is to use a sampling method like Top-K sampling, where instead of choosing the single most probable next word (greedy decoding), the model samples from the top K most likely next words. This can provide more diverse and possibly more accurate predictions when dealing with ambiguous contexts.\n",
    "\n",
    "Here’s how you can integrate Top-K sampling into your existing setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4da40271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Hello there,\n",
      "Predicted Next Word: my\n",
      "Context: Hello there, I hope you're doing well.\n",
      "Predicted Next Word: \n",
      "Context: Hello there, I hope you're doing well. Today, we're going to talk about streaming algorithms.\n",
      "Predicted Next Word: Let\n",
      "Context: Hello there, I hope you're doing well. Today, we're going to talk about streaming algorithms. Streaming algorithms process data in chunks,\n",
      "Predicted Next Word: so\n",
      "Context: Hello there, I hope you're doing well. Today, we're going to talk about streaming algorithms. Streaming algorithms process data in chunks, allowing for real-time processing and responses.\n",
      "Predicted Next Word: \n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Simulated text stream input\n",
    "text_stream = [\n",
    "    \"Hello there,\",\n",
    "    \"I hope you're doing well.\",\n",
    "    \"Today, we're going to talk about streaming algorithms.\",\n",
    "    \"Streaming algorithms process data in chunks,\",\n",
    "    \"allowing for real-time processing and responses.\"\n",
    "]\n",
    "\n",
    "# Initialize an empty context (state)\n",
    "context = \"\"\n",
    "\n",
    "def process_chunk(chunk, context, model, tokenizer, max_length=50, k=5):\n",
    "    \"\"\"\n",
    "    Processes a single chunk of text using a language model, updates the context, and generates output.\n",
    "    \"\"\"\n",
    "    # Update the context by appending the current chunk\n",
    "    new_context = context + \" \" + chunk\n",
    "\n",
    "    # Ensure the context doesn't exceed the max_length\n",
    "    tokens = tokenizer.tokenize(new_context)\n",
    "    tokens = tokens[-max_length:]  # Keep only the last max_length tokens\n",
    "    new_context = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "    # Tokenize the new context\n",
    "    inputs = tokenizer(new_context, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Generate predictions (inference) using Top-K sampling\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.topk(outputs.logits[:, -1, :], k=k)  # Get top-k logits\n",
    "\n",
    "    # Sample from the top-k predictions\n",
    "    predicted_token_ids = predictions.indices[0].tolist()\n",
    "    predicted_word = tokenizer.decode(predicted_token_ids[torch.randint(0, k, (1,))]).strip()\n",
    "\n",
    "    # Generate output based on the updated context\n",
    "    output = f\"Context: {new_context.strip()}\\nPredicted Next Word: {predicted_word}\"\n",
    "    \n",
    "    return new_context, output\n",
    "\n",
    "# Process each chunk in the text stream\n",
    "for chunk in text_stream:\n",
    "    context, output = process_chunk(chunk, context, model, tokenizer)\n",
    "    print(output)  # Print the incremental output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3c876",
   "metadata": {},
   "source": [
    "### **Part 2: Streaming Training (Online Learning)**\n",
    "\n",
    "#### **High-Level Intuition**\n",
    "\n",
    "Streaming training, also known as online learning, involves continuously updating a model as new data arrives. Instead of training the model on a fixed dataset, the model learns incrementally from each new chunk of data. This approach allows the model to adapt to changing environments or data distributions in real-time.\n",
    "\n",
    "#### **Example Use Case**\n",
    "\n",
    "Consider a news recommendation system that learns user preferences in real-time. As the user reads different articles, the system continuously updates its model based on the user's interactions, improving its recommendations over time. This ensures that the recommendations remain relevant as the user's interests evolve.\n",
    "\n",
    "#### **Core Algorithmic Details**\n",
    "\n",
    "1. **Incremental Learning:** The model is updated with each new chunk of data, refining its parameters based on the latest information.\n",
    "2. **Adaptive Model:** The model can adapt to changes in data distribution, such as shifts in user preferences or new trends.\n",
    "\n",
    "#### **Python Implementation**\n",
    "\n",
    "Below is a simple example of online learning using a linear regression model with stochastic gradient descent (SGD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e16b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for input [5, 6]: 7.287013911060262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Simulated stream of data (features and target)\n",
    "data_stream = [\n",
    "    (np.array([1, 2]), 5),\n",
    "    (np.array([2, 3]), 7),\n",
    "    (np.array([3, 4]), 9),\n",
    "    (np.array([4, 5]), 11)\n",
    "]\n",
    "\n",
    "# Initialize the model\n",
    "model = SGDRegressor()\n",
    "\n",
    "# Online learning: process each chunk of data\n",
    "for features, target in data_stream:\n",
    "    features = features.reshape(1, -1)  # Reshape for model input\n",
    "    model.partial_fit(features, [target])  # Update the model incrementally\n",
    "\n",
    "# Test the model with new data\n",
    "test_data = np.array([5, 6]).reshape(1, -1)\n",
    "prediction = model.predict(test_data)\n",
    "print(f\"Prediction for input [5, 6]: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a49000",
   "metadata": {},
   "source": [
    "### **Part 3: Simultaneous Inference and Training**\n",
    "\n",
    "#### **High-Level Intuition**\n",
    "\n",
    "Simultaneous inference and training involve performing both tasks concurrently. The model not only makes predictions (inference) based on incoming data but also updates its parameters (training) using the same data. This approach is beneficial in environments where the model needs to adapt quickly while continuing to provide outputs.\n",
    "\n",
    "#### **Example Use Case**\n",
    "\n",
    "In a real-time language translation system, the model could both translate incoming text and learn from user corrections. As users correct translations, the model updates its parameters to improve future translations, ensuring it adapts to specific language nuances or user preferences.\n",
    "\n",
    "#### **Core Algorithmic Details**\n",
    "\n",
    "1. **Concurrent Processing:** The model handles both inference and training simultaneously for each data chunk.\n",
    "2. **Continuous Adaptation:** The model continuously improves based on new data while still providing real-time outputs.\n",
    "\n",
    "#### **Python Implementation**\n",
    "\n",
    "Here’s a basic example combining inference and training using the same linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2e22fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: 0.3\n",
      "Prediction after training: 0.5371325788774437\n",
      "Prediction before training: 0.8056988472937551\n",
      "Prediction after training: 1.4646294520069894\n",
      "Prediction before training: 1.968528069532016\n",
      "Prediction after training: 3.2612471599319854\n",
      "Prediction before training: 4.1131855250098575\n",
      "Prediction after training: 6.04748887212382\n",
      "Final prediction for input [5, 6]: 7.313920804585054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Simulated stream of data (features and target)\n",
    "data_stream = [\n",
    "    (np.array([1, 2]), 5),\n",
    "    (np.array([2, 3]), 7),\n",
    "    (np.array([3, 4]), 9),\n",
    "    (np.array([4, 5]), 11)\n",
    "]\n",
    "\n",
    "# Initialize the model\n",
    "model = SGDRegressor()\n",
    "\n",
    "# Perform an initial training step with the first data point\n",
    "first_features, first_target = data_stream[0]\n",
    "first_features = first_features.reshape(1, -1)\n",
    "model.partial_fit(first_features, [first_target])\n",
    "\n",
    "# Simultaneous inference and training\n",
    "for features, target in data_stream:\n",
    "    features = features.reshape(1, -1)\n",
    "    \n",
    "    # Inference: Make a prediction\n",
    "    prediction = model.predict(features)\n",
    "    print(f\"Prediction before training: {prediction[0]}\")\n",
    "    \n",
    "    # Training: Update the model with new data\n",
    "    model.partial_fit(features, [target])\n",
    "    \n",
    "    # Inference after training (optional)\n",
    "    prediction_after_training = model.predict(features)\n",
    "    print(f\"Prediction after training: {prediction_after_training[0]}\")\n",
    "\n",
    "# Test the model with new data\n",
    "test_data = np.array([5, 6]).reshape(1, -1)\n",
    "final_prediction = model.predict(test_data)\n",
    "print(f\"Final prediction for input [5, 6]: {final_prediction[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2047c",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "\n",
    "In this tutorial, we've covered the essentials of streaming LLMs, focusing on inference, training (online learning), and simultaneous inference and training. We provided high-level intuitions, core algorithmic details, and simple Python implementations for each concept. Streaming LLMs enable real-time processing and continuous learning, making them powerful tools for dynamic environments where adaptability and immediacy are key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567cb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
